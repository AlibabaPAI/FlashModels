---
language:
- zh
- en
tags:
- qwen
pipeline_tag: text-generation
inference: false
---

# Qwen-7B

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo.jpg" width="400"/>
<p>
<br>

<p align="center">
        Qwen-7B <a href="https://modelscope.cn/models/qwen/Qwen-7B/summary">ğŸ¤– <a> | <a href="https://huggingface.co/Qwen/Qwen-7B">ğŸ¤—</a>&nbsp ï½œ Qwen-7B-Chat <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary">ğŸ¤– <a> | <a href="https://huggingface.co/Qwen/Qwen-7B-Chat">ğŸ¤—</a>&nbsp | Qwen-7B-Chat-Int4 <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int4">ğŸ¤—</a>
<br>
<a href="https://github.com/QwenLM/Qwen-7B/blob/main/assets/wechat.png">WeChat</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://modelscope.cn/studios/qwen/Qwen-7B-Chat-Demo/summary">Demo</a>&nbsp ï½œ &nbsp<a href="https://github.com/QwenLM/Qwen-7B/blob/main/tech_memo.md">Report</a>
</p>
<br>

## ä»‹ç» (Introduction)

**é€šä¹‰åƒé—®-7Bï¼ˆQwen-7Bï¼‰**æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„é€šä¹‰åƒé—®å¤§æ¨¡å‹ç³»åˆ—çš„70äº¿å‚æ•°è§„æ¨¡çš„æ¨¡å‹ã€‚Qwen-7Bæ˜¯åŸºäºTransformerçš„å¤§è¯­è¨€æ¨¡å‹, åœ¨è¶…å¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå¾—åˆ°ã€‚é¢„è®­ç»ƒæ•°æ®ç±»å‹å¤šæ ·ï¼Œè¦†ç›–å¹¿æ³›ï¼ŒåŒ…æ‹¬å¤§é‡ç½‘ç»œæ–‡æœ¬ã€ä¸“ä¸šä¹¦ç±ã€ä»£ç ç­‰ã€‚åŒæ—¶ï¼Œåœ¨Qwen-7Bçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹é½æœºåˆ¶æ‰“é€ äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„AIåŠ©æ‰‹Qwen-7B-Chatã€‚æœ¬ä»“åº“ä¸ºQwen-7Bçš„ä»“åº“ã€‚

é€šä¹‰åƒé—®-7Bï¼ˆQwen-7Bï¼‰ä¸»è¦æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **å¤§è§„æ¨¡é«˜è´¨é‡è®­ç»ƒè¯­æ–™**ï¼šä½¿ç”¨è¶…è¿‡2.2ä¸‡äº¿tokensçš„æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼ŒåŒ…å«é«˜è´¨é‡ä¸­ã€è‹±ã€å¤šè¯­è¨€ã€ä»£ç ã€æ•°å­¦ç­‰æ•°æ®ï¼Œæ¶µç›–é€šç”¨åŠä¸“ä¸šé¢†åŸŸçš„è®­ç»ƒè¯­æ–™ã€‚é€šè¿‡å¤§é‡å¯¹æ¯”å®éªŒå¯¹é¢„è®­ç»ƒè¯­æ–™åˆ†å¸ƒè¿›è¡Œäº†ä¼˜åŒ–ã€‚
2. **å¼ºå¤§çš„æ€§èƒ½**ï¼šQwen-7Båœ¨å¤šä¸ªä¸­è‹±æ–‡ä¸‹æ¸¸è¯„æµ‹ä»»åŠ¡ä¸Šï¼ˆæ¶µç›–å¸¸è¯†æ¨ç†ã€ä»£ç ã€æ•°å­¦ã€ç¿»è¯‘ç­‰ï¼‰ï¼Œæ•ˆæœæ˜¾è‘—è¶…è¶Šç°æœ‰çš„ç›¸è¿‘è§„æ¨¡å¼€æºæ¨¡å‹ï¼Œç”šè‡³åœ¨éƒ¨åˆ†æŒ‡æ ‡ä¸Šç›¸æ¯”æ›´å¤§å°ºå¯¸æ¨¡å‹ä¹Ÿæœ‰è¾ƒå¼ºç«äº‰åŠ›ã€‚å…·ä½“è¯„æµ‹ç»“æœè¯·è¯¦è§ä¸‹æ–‡ã€‚
3. **è¦†ç›–æ›´å…¨é¢çš„è¯è¡¨**ï¼šç›¸æ¯”ç›®å‰ä»¥ä¸­è‹±è¯è¡¨ä¸ºä¸»çš„å¼€æºæ¨¡å‹ï¼ŒQwen-7Bä½¿ç”¨äº†çº¦15ä¸‡å¤§å°çš„è¯è¡¨ã€‚è¯¥è¯è¡¨å¯¹å¤šè¯­è¨€æ›´åŠ å‹å¥½ï¼Œæ–¹ä¾¿ç”¨æˆ·åœ¨ä¸æ‰©å±•è¯è¡¨çš„æƒ…å†µä¸‹å¯¹éƒ¨åˆ†è¯­ç§è¿›è¡Œèƒ½åŠ›å¢å¼ºå’Œæ‰©å±•ã€‚

å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šå…³äºé€šä¹‰åƒé—®7Bå¼€æºæ¨¡å‹çš„ç»†èŠ‚ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨å‚é˜…[Githubä»£ç åº“](https://github.com/QwenLM/Qwen-7B)ã€‚

**Qwen-7B** is the 7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen-7B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-7B, we release Qwen-7B-Chat, a large-model-based AI assistant, which is trained with alignment techniques. This repository is the one for Qwen-7B.

The features of Qwen-7B include:

1. **Large-scale high-quality training corpora**: It is pretrained on over 2.2 trillion tokens, including Chinese, English, multilingual texts, code, and mathematics, covering general and professional fields. The distribution of the pre-training corpus has been optimized through a large number of ablation experiments.
2. **Competitive performance**: It significantly surpasses existing open-source models of similar scale on multiple Chinese and English downstream evaluation tasks (including commonsense, reasoning, code, mathematics, etc.), and even surpasses some larger-scale models in several benchmarks. See below for specific evaluation results.
3. **More comprehensive vocabulary coverage**: Compared with other open-source models based on Chinese and English vocabularies, Qwen-7B uses a vocabulary of over 150K tokens. This vocabulary is more friendly to multiple languages, enabling users to directly further enhance the capability for certain languages without expanding the vocabulary.

For more details about the open-source model of Qwen-7B, please refer to the [Github](https://github.com/QwenLM/Qwen-7B) code repository.
<br>

## è¦æ±‚ï¼ˆRequirementsï¼‰

* python 3.8åŠä»¥ä¸Šç‰ˆæœ¬
* pytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œæ¨è2.0åŠä»¥ä¸Šç‰ˆæœ¬
* å»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·ã€flash-attentionç”¨æˆ·ç­‰éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰
* python 3.8 and above
* pytorch 1.12 and above, 2.0 and above are recommended
* CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)
<br>

## ä¾èµ–é¡¹ (Dependency)

è¿è¡ŒQwen-7Bï¼Œè¯·ç¡®ä¿æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œå†æ‰§è¡Œä»¥ä¸‹pipå‘½ä»¤å®‰è£…ä¾èµ–åº“

To run Qwen-7B, please make sure you meet the above requirements, and then execute the following pip commands to install the dependent libraries.

```bash
pip install transformers==4.31.0 accelerate tiktoken einops
```

å¦å¤–ï¼Œæ¨èå®‰è£…`flash-attention`åº“ï¼Œä»¥å®ç°æ›´é«˜çš„æ•ˆç‡å’Œæ›´ä½çš„æ˜¾å­˜å ç”¨ã€‚

In addition, it is recommended to install the `flash-attention` library for higher efficiency and lower memory usage.

```bash
git clone -b v1.0.8 https://github.com/Dao-AILab/flash-attention
cd flash-attention && pip install .
# ä¸‹æ–¹å®‰è£…å¯é€‰ï¼Œå®‰è£…å¯èƒ½æ¯”è¾ƒç¼“æ…¢ã€‚
# Below are optional. Installing them might be slow.
# pip install csrc/layer_norm
# pip install csrc/rotary
```
<br>

## å¿«é€Ÿä½¿ç”¨ï¼ˆQuickstartï¼‰

æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç è½»æ¾è°ƒç”¨ï¼š

You can easily call the model with the following code:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# Note: The default behavior now has injection attack prevention off.
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)

# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="cpu", trust_remote_code=True).eval()
# use auto mode, automatically select precision based on the device.
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True).eval()

# Specify hyperparameters for generation
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)

inputs = tokenizer('è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯', return_tensors='pt')
inputs = inputs.to(model.device)
pred = model.generate(**inputs)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))
# è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯äºšçš„æ–¯äºšè´å·´ï¼ˆAddis Ababaï¼‰...
```

å…³äºæ›´å¤šçš„ä½¿ç”¨è¯´æ˜ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„[Github repo](https://github.com/QwenLM/Qwen-7B)è·å–æ›´å¤šä¿¡æ¯ã€‚

For more information, please refer to our [Github repo](https://github.com/QwenLM/Qwen-7B) for more information.
<br>

## Tokenizer

> æ³¨ï¼šä½œä¸ºæœ¯è¯­çš„â€œtokenizationâ€åœ¨ä¸­æ–‡ä¸­å°šæ— å…±è¯†çš„æ¦‚å¿µå¯¹åº”ï¼Œæœ¬æ–‡æ¡£é‡‡ç”¨è‹±æ–‡è¡¨è¾¾ä»¥åˆ©è¯´æ˜ã€‚

åŸºäºtiktokençš„åˆ†è¯å™¨æœ‰åˆ«äºå…¶ä»–åˆ†è¯å™¨ï¼Œæ¯”å¦‚sentencepieceåˆ†è¯å™¨ã€‚å°¤å…¶åœ¨å¾®è°ƒé˜¶æ®µï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„ç‰¹æ®Štokençš„ä½¿ç”¨ã€‚å…³äºtokenizerçš„æ›´å¤šä¿¡æ¯ï¼Œä»¥åŠå¾®è°ƒæ—¶æ¶‰åŠçš„ç›¸å…³ä½¿ç”¨ï¼Œè¯·å‚é˜…[æ–‡æ¡£](https://github.com/QwenLM/Qwen-7B/blob/main/tokenization_note_zh.md)ã€‚

Our tokenizer based on tiktoken is different from other tokenizers, e.g., sentencepiece tokenizer. You need to pay attention to special tokens, especially in finetuning. For more detailed information on the tokenizer and related use in fine-tuning, please refer to the [documentation](https://github.com/QwenLM/Qwen-7B/blob/main/tokenization_note.md).
<br>

## æ¨¡å‹ç»†èŠ‚ (Model)

Qwen-7Bæ¨¡å‹è§„æ¨¡åŸºæœ¬æƒ…å†µå¦‚ä¸‹æ‰€ç¤ºï¼š

The details of the model architecture of Qwen-7B are listed as follows:

| Hyperparameter  |  Value |
|:----------------|:-------|
|    n_layers     |     32 |
|     n_heads     |     32 |
|     d_model     |   4096 |
|   vocab size    | 151851 |
| sequence length |   2048 |

åœ¨ä½ç½®ç¼–ç ã€FFNæ¿€æ´»å‡½æ•°å’Œnormalizationçš„å®ç°æ–¹å¼ä¸Šï¼Œæˆ‘ä»¬ä¹Ÿé‡‡ç”¨äº†ç›®å‰æœ€æµè¡Œçš„åšæ³•ï¼Œ
å³RoPEç›¸å¯¹ä½ç½®ç¼–ç ã€SwiGLUæ¿€æ´»å‡½æ•°ã€RMSNormï¼ˆå¯é€‰å®‰è£…flash-attentionåŠ é€Ÿï¼‰ã€‚

åœ¨åˆ†è¯å™¨æ–¹é¢ï¼Œç›¸æ¯”ç›®å‰ä¸»æµå¼€æºæ¨¡å‹ä»¥ä¸­è‹±è¯è¡¨ä¸ºä¸»ï¼ŒQwen-7Bä½¿ç”¨äº†è¶…è¿‡15ä¸‡tokenå¤§å°çš„è¯è¡¨ã€‚ è¯¥è¯è¡¨åœ¨GPT-4ä½¿ç”¨çš„BPEè¯è¡¨`cl100k_base`åŸºç¡€ä¸Šï¼Œå¯¹ä¸­æ–‡ã€å¤šè¯­è¨€è¿›è¡Œäº†ä¼˜åŒ–ï¼Œåœ¨å¯¹ä¸­ã€è‹±ã€ä»£ç æ•°æ®çš„é«˜æ•ˆç¼–è§£ç çš„åŸºç¡€ä¸Šï¼Œå¯¹éƒ¨åˆ†å¤šè¯­è¨€æ›´åŠ å‹å¥½ï¼Œæ–¹ä¾¿ç”¨æˆ·åœ¨ä¸æ‰©å±•è¯è¡¨çš„æƒ…å†µä¸‹å¯¹éƒ¨åˆ†è¯­ç§è¿›è¡Œèƒ½åŠ›å¢å¼ºã€‚
è¯è¡¨å¯¹æ•°å­—æŒ‰å•ä¸ªæ•°å­—ä½åˆ‡åˆ†ã€‚è°ƒç”¨è¾ƒä¸ºé«˜æ•ˆçš„[tiktokenåˆ†è¯åº“](https://github.com/openai/tiktoken)è¿›è¡Œåˆ†è¯ã€‚

æˆ‘ä»¬ä»éƒ¨åˆ†è¯­ç§å„éšæœºæŠ½å–100ä¸‡ä¸ªæ–‡æ¡£è¯­æ–™ï¼Œä»¥å¯¹æ¯”ä¸åŒæ¨¡å‹çš„ç¼–ç å‹ç¼©ç‡ï¼ˆä»¥æ”¯æŒ100è¯­ç§çš„XLM-Rä¸ºåŸºå‡†å€¼1ï¼Œè¶Šä½è¶Šå¥½ï¼‰ï¼Œå…·ä½“æ€§èƒ½è§å›¾ã€‚

å¯ä»¥çœ‹åˆ°Qwen-7Båœ¨ä¿æŒä¸­è‹±ä»£ç é«˜æ•ˆè§£ç çš„å‰æä¸‹ï¼Œå¯¹éƒ¨åˆ†ä½¿ç”¨äººç¾¤è¾ƒå¤šçš„è¯­ç§ï¼ˆæ³°è¯­thã€å¸Œä¼¯æ¥è¯­heã€é˜¿æ‹‰ä¼¯è¯­arã€éŸ©è¯­koã€è¶Šå—è¯­viã€æ—¥è¯­jaã€åœŸè€³å…¶è¯­trã€å°å°¼è¯­idã€æ³¢å…°è¯­plã€ä¿„è¯­ruã€è·å…°è¯­nlã€è‘¡è„ç‰™è¯­ptã€æ„å¤§åˆ©è¯­itã€å¾·è¯­deã€è¥¿ç­ç‰™è¯­esã€æ³•è¯­frç­‰ï¼‰ä¸Šä¹Ÿå®ç°äº†è¾ƒé«˜çš„å‹ç¼©ç‡ï¼Œä½¿å¾—æ¨¡å‹åœ¨è¿™äº›è¯­ç§ä¸Šä¹Ÿå…·å¤‡è¾ƒå¼ºçš„å¯æ‰©å±•æ€§å’Œè¾ƒé«˜çš„è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚

åœ¨é¢„è®­ç»ƒæ•°æ®æ–¹é¢ï¼ŒQwen-7Bæ¨¡å‹ä¸€æ–¹é¢åˆ©ç”¨äº†éƒ¨åˆ†å¼€æºé€šç”¨è¯­æ–™ï¼Œ
å¦ä¸€æ–¹é¢ä¹Ÿç§¯ç´¯äº†æµ·é‡å…¨ç½‘è¯­æ–™ä»¥åŠé«˜è´¨é‡æ–‡æœ¬å†…å®¹ï¼Œå»é‡åŠè¿‡æ»¤åçš„è¯­æ–™è¶…è¿‡2.2T tokensã€‚
å›Šæ‹¬å…¨ç½‘æ–‡æœ¬ã€ç™¾ç§‘ã€ä¹¦ç±ã€ä»£ç ã€æ•°å­¦åŠå„ä¸ªé¢†åŸŸå‚ç±»ã€‚

<p align="center">
    <img src="assets/tokenizer.png" style="width: 1200px"/>
<p>

For position encoding, FFN activation function, and normalization methods, we adopt the prevalent practices, i.e., RoPE relative position encoding, SwiGLU for activation function, and RMSNorm for normalization (optional installation of flash-attention for acceleration).

For tokenization, compared to the current mainstream open-source models based on Chinese and English vocabularies, Qwen-7B uses a vocabulary of over 150K tokens. It first considers efficient encoding of Chinese, English, and code data, and is also more friendly to multilingual languages, enabling users to directly enhance the capability of some languages without expanding the vocabulary. It segments numbers by single digit, and calls the [tiktoken](https://github.com/openai/tiktoken) tokenizer library for efficient tokenization.

We randomly selected 1 million document corpus of each language to test and compare the encoding compression rates of different models (with XLM-R, which supports 100 languages, as the base value 1). The specific performance is shown in the figure above.

As can be seen, while ensuring the efficient decoding of Chinese, English, and code, Qwen-7B also achieves a high compression rate for many other languages (such as th, he, ar, ko, vi, ja, tr, id, pl, ru, nl, pt, it, de, es, fr etc.), equipping the model with strong scalability as well as high training and inference efficiency in these languages.

For pre-training data, on the one hand, Qwen-7B uses part of the open-source generic corpus. On the other hand, it uses a massive amount of accumulated web corpus and high-quality text content. The scale of corpus reaches over 2.2T tokens after deduplication and filtration, encompassing web text, encyclopedias, books, code, mathematics, and various domain.
<br>

## è¯„æµ‹æ•ˆæœï¼ˆEvaluationï¼‰

### ä¸­æ–‡è¯„æµ‹ï¼ˆChinese Evaluationï¼‰

#### C-Eval

[C-Eval](https://arxiv.org/abs/2305.08322)æ˜¯è¯„æµ‹é¢„è®­ç»ƒæ¨¡å‹ä¸­æ–‡å¸¸è¯†èƒ½åŠ›çš„å¸¸ç”¨æµ‹è¯„æ¡†æ¶ï¼Œè¦†ç›–äººæ–‡ã€ç¤¾ç§‘ã€ç†å·¥ã€å…¶ä»–ä¸“ä¸šå››ä¸ªå¤§æ–¹å‘å…±52ä¸ªå­¦ç§‘ã€‚
æˆ‘ä»¬æŒ‰ç…§æ ‡å‡†åšæ³•ï¼Œä»¥å¼€å‘é›†æ ·æœ¬ä½œä¸ºfew-shotæ¥æºï¼Œè¯„ä»·Qwen-7Bé¢„è®­ç»ƒæ¨¡å‹çš„5-shotéªŒè¯é›†ä¸æµ‹è¯•é›†å‡†ç¡®ç‡ã€‚

[C-Eval](https://arxiv.org/abs/2305.08322) is a common evaluation benchmark for testing the common sense capability of pre-trained models in Chinese. It covers 52 subjects in four major directions: humanities, social sciences, STEM, and other specialties. According to the standard practice, we use the development set samples as the source of few-shot, to evaluate the 5-shot validation set and test set accuracy of the Qwen-7B pre-trained model.

åœ¨C-EvaléªŒè¯é›†ä¸Šï¼ŒQwen-7Bæ¨¡å‹å’Œå…¶ä»–æ¨¡å‹çš„å‡†ç¡®ç‡å¯¹æ¯”å¦‚ä¸‹ï¼š

The accuracy comparison of Qwen-7B and the other models on the C-Eval validation set is shown as follows:

|      Model      |     Avg. |
|:----------------|:--------:|
|    Alpaca-7B    |     28.9 |
|    Vicuna-7B    |     31.2 |
|   ChatGLM-6B    |     37.1 |
|   Baichuan-7B   |     42.7 |
|   ChatGLM2-6B   |     50.9 |
|   InternLM-7B   |     53.4 |
|     ChatGPT     |     53.5 |
|   Claude-v1.3   |     55.5 |
|   **Qwen-7B**   | **60.8** |

åœ¨C-Evalæµ‹è¯•é›†ä¸Šï¼ŒQwen-7Bé¢„è®­ç»ƒæ¨¡å‹ä¸å…¶ä»–æ¨¡å‹çš„æ•ˆæœå¯¹æ¯”å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š

The performance comparison of Qwen-7B and other models on the C-Eval test set is shown in the following table:

| Model                   |   Avg.   | Avg. (Hard) | STEM   | Social Sciences | Humanities | Others |
|:------------------------|:--------:|:-----------:|:------:|:---------------:|:----------:|:------:|
| ChatGLM-6B              |   38.9   |     29.2    |  33.3  |       48.3      |    41.3    |  38.0  |
| Chinese-Alpaca-Plus-13B |   41.5   |     30.5    |  36.6  |       49.7      |    43.1    |  41.2  |
| Baichuan-7B             |   42.8   |     31.5    |  38.2  |       52.0      |    46.2    |  39.3  |
| WestlakeLM-19B          |   44.6   |     34.9    |  41.6  |       51.0      |    44.3    |  44.5  |
| AndesLM-13B             |   46.0   |     29.7    |  38.1  |       61.0      |    51.0    |  41.9  |
| BatGPT-15B-sirius       |   47.0   |     31.9    |  42.7  |       57.5      |    48.6    |  43.6  |
| ChatGLM2-6B             |   51.7   |     37.1    |  48.6  |       60.5      |    51.3    |  49.8  |
| InternLM-7B             |   52.8   |     37.1    |  48.0  |       67.4      |    55.4    |  45.8  |
| Baichuan-13B            |   53.6   |     36.7    |  47.0  |       66.8      |    57.3    |  49.8  |
| Claude-v1.3             |   54.2   |     39.0    |  51.9  |       61.7      |    52.1    |  53.7  |
| ChatGPT                 |   54.4   |     41.4    |  52.9  |       61.8      |    50.9    |  53.6  |
| **Qwen-7B**             | **59.6** |     41.0    |  52.8  |       74.1      |    63.1    |  55.2  |

å¯ä»¥çœ‹åˆ°ï¼ŒQwen-7Båœ¨åŒç­‰è§„æ¨¡ç°æœ‰æ¨¡å‹ä¸­å–å¾—äº†æœ€é«˜çš„åˆ†æ•°ï¼Œç”šè‡³ç›¸æ¯”æ›´å¤§è§„æ¨¡æ¨¡å‹ä¹Ÿå…·æœ‰è¾ƒå¼ºç«äº‰åŠ›ã€‚

As can be seen, Qwen-7B achieves the best performance out of all existing models with similar scale and even surpasses larger-scale models.

### è‹±æ–‡è¯„æµ‹ï¼ˆEnglish Evaluationï¼‰

#### MMLU

[MMLU](https://arxiv.org/abs/2009.03300)æ˜¯ç›®å‰è¯„æµ‹è‹±æ–‡ç»¼åˆèƒ½åŠ›æœ€æƒå¨çš„åŸºå‡†è¯„æµ‹ä¹‹ä¸€ï¼ŒåŒæ ·è¦†ç›–äº†ä¸åŒå­¦ç§‘é¢†åŸŸã€ä¸åŒéš¾åº¦å±‚çº§çš„57ä¸ªå­ä»»åŠ¡ã€‚

Qwen-7Båœ¨MMLU 5-shotå‡†ç¡®ç‡è¡¨ç°å¦‚ä¸‹è¡¨ï¼š

[MMLU](https://arxiv.org/abs/2009.03300) is currently one of the most recognized benchmarks for evaluating English comprehension abilities, covering 57 subtasks across different academic fields and difficulty levels. The MMLU 5-shot accuracy performance of Qwen-7B is shown in the following table:

|     Model     |   Avg.   | STEM | Social Sciences | Humanities | Others |
|:--------------|:--------:|:----:|:---------------:|:----------:|:------:|
|  LLaMA-7B     |   35.1   | 30.5 |       38.3      |    34.0    |  38.1  |
|  Baichuan-7B  |   42.3   | 35.6 |       48.9      |    38.4    |  48.1  |
|  LLaMA2-7B    |   45.3   | 36.4 |       51.2      |    42.9    |  52.2  |
|  LLaMA-13B    |   46.9   | 35.8 |       53.8      |    45.0    |  53.3  |
|  ChatGLM2-6B  |   47.9   | 41.2 |       54.4      |    43.7    |  54.5  |
|  InternLM-7B  |   51.0   |   -  |         -       |      -     |    -   |
|  Baichuan-13B |   51.6   | 41.6 |       60.9      |    47.4    |  58.5  |
|  LLaMA2-13B   |   54.8   | 44.1 |       62.6      |    52.8    |  61.1  |
|  ChatGLM2-12B |   56.2   | 48.2 |       65.1      |    52.6    |  60.9  |
|  **Qwen-7B**  | **56.7** | 47.6 |       65.9      |    51.5    |  64.7  |

åœ¨è‹±æ–‡æ–¹é¢ï¼ŒQwen-7Bçš„æ•ˆæœåŒæ ·è¶…è¿‡äº†ç›®å‰å›½å†…å¤–å…¶ä»–åŒç±»å¼€æºé¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ ·å¯¹æ¯”æ›´å¤§è§„æ¨¡ç‰ˆæœ¬çš„æ¨¡å‹ä¹Ÿå…·æœ‰è¾ƒå¼ºç«äº‰åŠ›ã€‚

In terms of English, Qwen-7B also surpasses other similar open-source pre-trained models, and is competitive when compared to larger versions of other models.

### ä»£ç è¯„æµ‹ï¼ˆCoding Evaluationï¼‰

æˆ‘ä»¬åœ¨[HumanEval](https://github.com/openai/human-eval)ï¼ˆ0-shotï¼‰ä¸Šå¯¹æ¯”é¢„è®­ç»ƒæ¨¡å‹çš„ä»£ç èƒ½åŠ›ï¼Œç»“æœå¦‚ä¸‹ï¼š

We compared the code capabilities of pre-trained models on [HumanEval](https://github.com/openai/human-eval), and the results are as follows:

| Model         |  Pass@1  |
|:--------------|:--------:|
| Baichuan-7B   |   9.2    |
| ChatGLM2-6B   |   9.2    |
| InternLM-7B   |   10.4   |
| LLaMA-7B      |   10.5   |
| LLaMA2-7B     |   12.8   |
| Baichuan-13B  |   12.8   |
| LLaMA-13B     |   15.8   |
| MPT-7B        |   18.3   |
| LLaMA2-13B    |   18.3   |
| **Qwen-7B**   | **24.4** |

### æ•°å­¦è¯„æµ‹ï¼ˆMathematics Evaluationï¼‰

æ•°å­¦èƒ½åŠ›ä½¿ç”¨å¸¸ç”¨çš„[GSM8K](https://github.com/openai/grade-school-math)æ•°æ®é›†ï¼ˆ8-shotï¼‰è¯„ä»·ï¼š

We compared the math capabilities of pre-trained models on [GSM8K](https://github.com/openai/grade-school-math) (8-shot), and the results are as follows:

| Model         |   Acc.   |
|:--------------|:--------:|
| MPT-7B        |   6.8    |
| Falcon-7B     |   6.8    |
| Baichuan-7B   |   9.7    |
| LLaMA-7B      |   11.0   |
| LLaMA2-7B     |   14.6   |
| LLaMA-13B     |   17.8   |
| Baichuan-13B  |   26.6   |
| LLaMA2-13B    |   28.7   |
| InternLM-7B   |   31.2   |
| ChatGLM2-6B   |   32.4   |
| ChatGLM2-12B  |   40.9   |
| **Qwen-7B**   | **51.6** |

### ç¿»è¯‘è¯„æµ‹ï¼ˆTranslation Evaluationï¼‰

æˆ‘ä»¬ä½¿ç”¨[WMT22](https://www.statmt.org/wmt22/translation-task.html)ä¸­-è‹±ï¼ˆzh-enï¼‰å’Œè‹±-ä¸­ï¼ˆen-zhï¼‰æ•°æ®é›†ï¼ˆ5-shot BLEUï¼‰è¯„æµ‹ï¼š

We compared the translation capabilities of pre-trained models on [WMT22](https://www.statmt.org/wmt22/translation-task.html) zh-en and en-zh (5-shot BLEU), and the results are as follows:

|    Model    |     Avg. |    zh-en |    en-zh |
|:------------|:--------:|:--------:|:--------:|
| InternLM-7B |     11.8 |      9.0 |     14.5 |
|  LLaMA-7B   |     12.7 |     16.7 |      8.7 |
|  LLaMA-13B  |     15.8 |     19.5 |     12.0 |
|  LLaMA2-7B  |     19.9 |     21.9 |     17.9 |
|  Bloom-7B   |     20.3 |     19.1 |     21.4 |
| LLaMA2-13B  |     23.3 |     22.4 |     24.2 |
| PolyLM-13B  |     23.6 |     20.2 |     27.0 |
| Baichuan-7B |     24.6 |     22.6 |     26.6 |
| **Qwen-7B** | **27.5** | **24.3** | **30.6** |

### é•¿åºåˆ—è¯„æµ‹ï¼ˆLong-Context Evaluationï¼‰

æˆ‘ä»¬å¼•å…¥NTKæ’å€¼ï¼ŒLogNæ³¨æ„åŠ›ç¼©æ”¾ï¼Œçª—å£æ³¨æ„åŠ›ç­‰æŠ€å·§ï¼Œå°†æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•åˆ°8Kä»¥ä¸Šã€‚åœ¨arXivæ•°æ®ä¸Šä½¿ç”¨PPLæŒ‡æ ‡æµ‹è¯•Qwen-7Båœ¨ä¸åŒé•¿åº¦ä¸‹çš„è¡¨ç°ï¼Œç»“æœå¦‚ä¸‹ï¼š

**(è‹¥è¦å¯ç”¨NTKå’ŒLogNæ³¨æ„åŠ›ç¼©æ”¾ï¼Œè¯·å°†config.jsoné‡Œçš„`use_dynamic_ntk`å’Œ`use_logn_attn`è®¾ç½®ä¸ºtrue)**

We introduce NTK-aware interpolation, LogN attention scaling, Window attention, etc. to extend the context length to over 8K tokens. We conduct language modeling experiments on the arXiv dataset with the PPL evaluation. Results are demonstrated below:

**(To use NTK interpolation and LogN scaling, please set `use_dynamic_ntk` and `use_long_attn` to true in config.json.)**

<table>
    <tr>
        <th rowspan="2">Model</th><th colspan="5" align="center">åºåˆ—é•¿åº¦ Sequence Length</th>
    </tr>
    <tr>
        <th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th><th align="center">16384</th>
    </tr>
    <tr>
        <td>Qwen-7B</td><td align="center"><b>4.23</b></td><td align="center"><b>3.78</b></td><td align="center">39.35</td><td align="center">469.81</td><td align="center">2645.09</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk</td><td align="center"><b>4.23</b></td><td align="center"><b>3.78</b></td><td align="center">3.59</td><td align="center">3.66</td><td align="center">5.71</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn</td><td align="center"><b>4.23</b></td><td align="center"><b>3.78</b></td><td align="center"><b>3.58</b></td><td align="center">3.56</td><td align="center">4.62</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>4.23</b></td><td align="center"><b>3.78</b></td><td align="center"><b>3.58</b></td><td align="center"><b>3.49</b></td><td align="center"><b>4.32</b></td>
    </tr>
</table>
<br>

## è¯„æµ‹å¤ç°ï¼ˆReproductionï¼‰

æˆ‘ä»¬æä¾›äº†è¯„æµ‹è„šæœ¬ï¼Œæ–¹ä¾¿å¤§å®¶å¤ç°æ¨¡å‹æ•ˆæœï¼Œè¯¦è§[é“¾æ¥](https://github.com/QwenLM/Qwen-7B/tree/main/eval)ã€‚æç¤ºï¼šç”±äºç¡¬ä»¶å’Œæ¡†æ¶é€ æˆçš„èˆå…¥è¯¯å·®ï¼Œå¤ç°ç»“æœå¦‚æœ‰å°å¹…æ³¢åŠ¨å±äºæ­£å¸¸ç°è±¡ã€‚

We have provided evaluation scripts to reproduce the performance of our model, details as [link](https://github.com/QwenLM/Qwen-7B/tree/main/eval).
<br>

## FAQ

å¦‚é‡åˆ°é—®é¢˜ï¼Œæ•¬è¯·æŸ¥é˜…[FAQ](https://github.com/QwenLM/Qwen-7B/blob/main/FAQ_zh.md)ä»¥åŠissueåŒºï¼Œå¦‚ä»æ— æ³•è§£å†³å†æäº¤issueã€‚

If you meet problems, please refer to [FAQ](https://github.com/QwenLM/Qwen-7B/blob/main/FAQ.md) and the issues first to search a solution before you launch a new issue.
<br>

## ä½¿ç”¨åè®®ï¼ˆLicense Agreementï¼‰

æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¹¶æ”¯æŒå•†ç”¨ã€‚è¯·æŸ¥çœ‹[LICENSE](https://github.com/QwenLM/Qwen-7B/blob/main/LICENSE)äº†è§£å…·ä½“çš„å¼€æºåè®®ç»†èŠ‚ã€‚å¦‚éœ€å•†ç”¨ï¼Œè¯·å¡«å†™[é—®å·](https://dashscope.console.aliyun.com/openModelApply/qianwen)ç”³è¯·ã€‚

Our code and checkpoints are open to research purpose, and they are allowed for commercial purposes. Check [LICENSE](https://github.com/QwenLM/Qwen-7B/blob/main/LICENSE) for more details about the license. If you have requirements for commercial use, please fill out the [form](https://dashscope.console.aliyun.com/openModelApply/qianwen) to apply.
<br>

## è”ç³»æˆ‘ä»¬ï¼ˆContact Usï¼‰

å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œè¯·é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚

If you are interested to leave a message to either our research team or product team, feel free to send an email to qianwen_opensource@alibabacloud.com.

